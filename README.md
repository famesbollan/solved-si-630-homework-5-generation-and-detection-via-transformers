Download Link: https://assignmentchef.com/product/solved-si-630-homework-5-generation-and-detection-via-transformers
<br>
Pretrained language models like BERT and ELMo have revolutionized many areas of NLP. These models have been trained on massive volumes of text and their parameters reflect a basic understanding of the structure and meaning of many kinds of text. The killer application for such models is ​<em>fine-tuning</em>​, where the parameters are modified to reflect a particular type of text (e.g., social media) and to use them to perform some specific task (e.g., classification). As a result, with just a limited amount of data, these models are capable of generalizing to a much wider set of instances; for example, a BERT-based classifier trained on a few thousand classifiers might perform substantially better than a logistic regression classifier trained on the same data. Homework 5 will get you experience with these kinds of models.

In the long long ago of Lecture 2, we learned about language models that could generate text and briefly played around with the Talk to Transformer website,<sup>1</sup> which can generate stories from the start of one. Wouldn’t it be cool to try building and using that kind of technology yourself? In this assignment, you’ll finally get face to face with some of the latest NLP deep learning libraries to do two tasks (1) generate the start of a short story and (2) detect whether a short story you’ve been given was generated by a machine or not. As a part of this assignment, you’ll work with the HuggingFace transformers library which has implementations of many of the latest-and-greatest NLP models and, ​<em>conveniently</em>​, has included example scripts for how to effectively fine-tune pretrained models to do both of these tasks.




The overarching goal of Homework 5 is to generate as realistic of a story start as you can from the generation model. However, you’re also tasked with detecting these machine-generated stories, which creates a dilemma–you want your generation system to fool your classification system (making it think the stories are human), yet you also want the classification system to do well.<sup>2</sup>




Unlike previous homeworks, ​<strong>Homework 5 is a two-person homework</strong>​. The intent is not that it’s difficult (it’s not) but that by dividing up the work for generation and classification, you can try

<sup>1</sup>2 ​https://talktotransformer.com/

​We’ll talk about this concept much more in class as a part of a special kind of network called a Generative Adversarial Network (GAN) but you are ​<em>not</em>​ implementing that here.

testing the generation output on the classifier to see if it can fool it. Most of the homework effort will consist of familiarizing yourself with how to run these models and will likely not involve writing much code (unless you want to go wild on creating cool generators/classifiers, which I fully support).




Homework 5 has the following (very practical) learning goals:

<ol>

 <li>Learn how to use modern deep learning libraries like transformers​ to fine-tune​ pretrained models like BERT (for classification) and GPT (for generation)</li>

 <li>Learn how to run your code on a GPU (as provided by Great Lakes)</li>

 <li>Try your hand at tuning hyperparameters to make better models</li>

</ol>

In summary, we want to help you get familiar with the basics of these models in a way that gives you skills you might use on a job, a course project, or in a research setting (e.g., fine-tuning a BERT classifier for your particular domain).




<h1>Part 1: Generating The Start of a Story</h1>

For Part 1, you’ll fine-tune one of the transformers from the HuggingFace library<a href="#_ftn1" name="_ftnref1"><sup>[1]</sup></a> to generate text. I personally recommend the openai-gpt​ model, as it’s small enough to reliably fit on the GPUs​   given to you on the Great Lakes clusters.<a href="#_ftn2" name="_ftnref2"><sup>[2]</sup></a> The library provides a sample script in the examples directory called run_language_model.py which you can/should use to do your training.




We have provided example stories from various sources for you to use in three files in the data/generation/ directory: train.txt, dev.txt, test.txt. You should use the train.txt to fine-tune your model, choosing hyperparameters that converge to a low loss. To figure out the right hyperparameters, you should use the dev.txt to select the best hyperparameters that give the lowest perplexity (reminder: perplexity is the metric we discussed back in Lecture 2 that measure how surprised the model is by the words that come next; a lower perplexity indicates the language model expects to be generating this kind of word, which is what we want).




What to do:

<ul>

 <li>Train a model using train.txt and save it to some output directory. You can/should train multiple models and evaluate them using dev.txt to pick the model with the lowest perplexity on the dev.txt set.</li>

 <li>Report the following

  <ol>

   <li>The loss on train.txt for your best model and its perplexity (on train.txt)</li>

   <li>the perplexity on dev.txt</li>

   <li><strong>once you have finalized the model</strong>, the perplexity on test.txt​</li>

  </ol></li>

 <li>Using your final model and random seed 9001,<a href="#_ftn3" name="_ftnref3"><sup>[3]</sup></a> Generate story starts of length 32 for the following 10 prompts:

  <ol>

   <li>My</li>

   <li>The</li>

   <li>One</li>

   <li>When</li>

   <li>If</li>

   <li>Our</li>

   <li>First</li>

   <li>Natural</li>

   <li>We</li>

   <li>Because</li>

  </ol></li>

</ul>




Implementation notes:

<ul>

 <li>You’re welcome to add data to train.txt should you want to go off and collect more. Go wild with stories from different domains, styles, authors, etc. The world is your textual oyster.</li>

 <li>One of the big slow-downs in fine-tuning is converting the text into features, which takes 5-10 minutes. However, if you’re using the same model over and over, the code will save these featurized data to a pickle and save lots of time.</li>

 <li>The block_size parameter specifies the maximum length of any sequence. With a smaller block size, you can fit more instances in a batch (potentially giving better convergence) but with a larger sequence, you get more text for the model to learn from. It’s a trade-off you’ll need to explore.</li>

</ul>

<h1>Part 2: Training a classifier to recognize machine-generated text</h1>

Part 2 will develop a classifier to recognize machine-generated text. You’ll once again use the transformers library to train a model. I personally recommend the distilbert-base-cased​ model, as it’s small enough to reliably fit on the GPUs given to you on the Great Lakes clusters. This particular model is a pared-down version of BERT with fewer parameters, however you’re welcomed to try out different models!

The huggingface library doesn’t provide a script to train this off the shelf. However, there is a fantastic example notebook that will walk you through all the steps to train a classifier here: <u><a href="https://colab.research.google.com/drive/1pTuQhug6Dhl9XalKB0zUGf4FIdYFlpcX">https://colab.research.google.com/drive/1pTuQhug6Dhl9XalKB0zUGf4FIdYFlpcX</a></u> <u>​ </u>In particular, your task will involve two main steps:

<ol>

 <li>Pretrain your BERT model to recognize the language of short stories</li>

 <li>Train a classifier on the encoded to text</li>

</ol>

For #1, you should use the training data from the data/generation/ directory to fine-tune your model. For #2, you’ll use the three provided files in the data/classification/ : train.json.txt, dev.json.txt, and test.json.txt.

You will need to write some code to load the data yourself and assign the classification label of each sentence (check out 3.3 in the example notebook for where to get started). This is the majority of the code you’ll need to write for this homework (aside from code that is copied from this notebook).




Important note: Be sure to use a sequence length of 32 when training your classifier, as we’re going to set up our prediction task to only look at the first 32 words of a story to decide whether it was written by humans or machines.




What to do:

<ul>

 <li>Fine-tune your BERT model (or the DistilBERT model) and save that to a directory</li>

 <li>Using your fine-tuned model, train the same model for classification using the train.txt in data/classification and save it to some output directory. You can/should train multiple models and evaluate them using dev.txt to pick the model with the best classification accuracy.</li>

 <li>Report the following

  <ol>

   <li>The accuracy on train.txt for your best model</li>

   <li>the accuracy on dev.txt</li>

   <li><strong>once you have finalized the classification model</strong>,​</li>

  </ol></li>

</ul>

<strong>■ </strong>the accuracy on test.txt

■  the accuracy on the 10 sentences you have generated from Part 2




<a href="#_ftnref1" name="_ftn1">[1]</a> https://github.com/huggingface/transformers

<a href="#_ftnref2" name="_ftn2">[2]</a> for those with more powerful GPUs, maybe try​ ​gpt2

<a href="#_ftnref3" name="_ftn3">[3]</a> Technical <u>​</u><u><a href="https://www.youtube.com/watch?v=SiMHTK15Pik">justification</a></u>​.